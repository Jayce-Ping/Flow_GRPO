{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2b2ec4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from itertools import chain, groupby\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57532c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_grid_info(prompt) -> tuple[int, int]:\n",
    "    # Grid can be represented as int x int, or int ⨉ int. ⨉ has unicode \\u2a09\n",
    "    match = re.findall(r'(\\d+)\\s*[x⨉]\\s*(\\d+)', prompt)\n",
    "    if len(match) == 0:\n",
    "        return (1, 1)\n",
    "\n",
    "    return (int(match[0][0]), int(match[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "411cc001",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/T2IS/T2IS_data_summary.jsonl', 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "with open('dataset/T2IS/generated_data_summary.jsonl', 'r') as f:\n",
    "    gen_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1a12399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_data = []\n",
    "for i, item in enumerate(data):\n",
    "    prompt = item[\"prompt\"]\n",
    "    grid_info = extract_grid_info(prompt)\n",
    "    simplified_item = {'idx': str(i).zfill(4)}\n",
    "    simplified_item.update({\n",
    "        k: v for k, v in item.items() if k in [\"prompt\", \"height\", 'width']\n",
    "    })\n",
    "    simplified_item['layout'] = 'x'.join(map(str, grid_info))\n",
    "    simplified_item['theme'] = item['instruction']\n",
    "    simplified_item.update(\n",
    "        {k:item[k] for k in ['category', 'task_name', 'criteria']}\n",
    "    )\n",
    "    simplified_item['from'] = 'T2IS'\n",
    "    simplified_item['original_idx'] = item['idx']\n",
    "    simplified_data.append(simplified_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f32130ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(gen_data, len(simplified_data)):\n",
    "    grid_info = item['layout'].split('x')\n",
    "    height = int(grid_info[0]) * 512\n",
    "    width = int(grid_info[1]) * 512\n",
    "    new_item : dict = {'idx': str(i).zfill(4)}\n",
    "    new_item.update({\n",
    "        k: item[k] for k in ['prompt']\n",
    "    })\n",
    "    new_item['height'] = height\n",
    "    new_item['width'] = width\n",
    "    new_item['layout'] = item['layout']\n",
    "    new_item['theme'] = item['theme']\n",
    "    new_item.update(\n",
    "        {k:item[k] for k in ['category', 'task_name', 'criteria']}\n",
    "    )\n",
    "    new_item['from'] = 'Generated'\n",
    "    new_item['original_idx'] = str(i - len(simplified_data)).zfill(4)\n",
    "    simplified_data.append(new_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "236a88d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/T2IS/extended_data_summary.jsonl', 'w') as f:\n",
    "    for item in simplified_data:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd249239",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/T2IS/extended_data_summary.jsonl', 'r') as f:\n",
    "    loaded_data = [json.loads(line) for line in f]\n",
    "\n",
    "loaded_data = {item['original_idx']: item for item in loaded_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3b69d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'train_half_leq_6'\n",
    "dataset_dir = f'dataset/T2IS/{label}'\n",
    "for file in ['train_metadata.jsonl', 'test_metadata.jsonl']:\n",
    "    with open(os.path.join(dataset_dir, file), 'r') as f:\n",
    "        metadata = [json.loads(line) for line in f]\n",
    "    \n",
    "    metadata = [\n",
    "        loaded_data[item['idx']] for item in metadata\n",
    "    ]\n",
    "\n",
    "    with open(os.path.join(dataset_dir, file), 'w') as f:\n",
    "        for item in metadata:\n",
    "            f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0933e67f",
   "metadata": {},
   "source": [
    "# Split Train/Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0d303870",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/T2IS/extended_data_summary.jsonl', 'r') as f:\n",
    "    data_ext = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a4d6aaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out items with resolution\n",
    "grid_limit = 5\n",
    "resolution_limit = grid_limit * 512 * 512\n",
    "data_ext = [\n",
    "    item for item in data_ext\n",
    "    if (item['height'] * item['width']) <= resolution_limit\n",
    "]\n",
    "\n",
    "data_ext = [\n",
    "    item for item in data_ext\n",
    "    if item['layout'] == '2x2'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6628a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "ratio = 0.5\n",
    "np.random.seed(seed)\n",
    "# Sample `ratio` of the data for training and rest for testing\n",
    "train_indices = np.random.choice(len(data_ext), size=int(len(data_ext)*ratio), replace=False)\n",
    "train_data = [data_ext[i] for i in train_indices]\n",
    "test_data = [data_ext[i] for i in range(len(data_ext)) if i not in train_indices]\n",
    "train_set = pd.DataFrame(train_data)\n",
    "test_set = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b62a8832",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_train = train_set.groupby(['category', 'task_name', 'layout'])\n",
    "grouped_test = test_set.groupby(['category', 'task_name', 'layout'])\n",
    "\n",
    "grouped_train = {\n",
    "    name: group.to_dict('records')\n",
    "    for name, group in grouped_train\n",
    "}\n",
    "grouped_test = {\n",
    "    name: group.to_dict('records')\n",
    "    for name, group in grouped_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "711c08d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 27)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grouped_train), len(grouped_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "62a6f3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1,  4,  5,  6,  7,  8,  9, 10, 11, 12, 14, 15, 16]),\n",
       " array([ 3,  5,  6,  7,  8,  9, 10, 11, 13, 14, 15, 17])]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.unique([len(x) for x in grouped.values()]) for grouped in [grouped_train, grouped_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed1c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"dataset/T2IS/train_extended_half_leq_{grid_limit}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "with open(os.path.join(output_dir, \"train_metadata.jsonl\"), \"w\") as f:\n",
    "    for group in grouped_train.values():\n",
    "        for item in group:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "with open(os.path.join(output_dir, \"test_metadata.jsonl\"), \"w\") as f:\n",
    "    for group in grouped_test.values():\n",
    "        for item in group:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a960e106",
   "metadata": {},
   "source": [
    "# Check criteria num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fb9fa229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_half/train_metadata.jsonl: [2 3 4 5]\n",
      "train_half/test_metadata.jsonl: [2 3 4 5]\n",
      "train_half_leq_5/train_metadata.jsonl: [2 3 4 5]\n",
      "train_half_leq_5/test_metadata.jsonl: [2 3 4 5]\n",
      "train_half_leq_4/train_metadata.jsonl: [2 3 4]\n",
      "train_half_leq_4/test_metadata.jsonl: [2 3 4 5]\n",
      "train_extended_half_leq_5/train_metadata.jsonl: [2 3 4 5]\n",
      "train_extended_half_leq_5/test_metadata.jsonl: [2 3 4 5]\n",
      "train_half_2by2/train_metadata.jsonl: [2 3 4]\n",
      "train_half_2by2/test_metadata.jsonl: [2 3 4 5]\n",
      "train_half_leq_6/train_metadata.jsonl: [2 3 4 5]\n",
      "train_half_leq_6/test_metadata.jsonl: [2 3 4 5]\n",
      "train_all_2by2/train_metadata.jsonl: [2 3 4 5]\n",
      "train_all_2by2/test_metadata.jsonl: [2 3 4]\n",
      "train_extended_half_2by2/train_metadata.jsonl: [2 3 4]\n",
      "train_extended_half_2by2/test_metadata.jsonl: [2 3 4 5]\n",
      "train_all/train_metadata.jsonl: [2 3 4 5]\n",
      "train_all/test_metadata.jsonl: [2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "def check_criteria_num(file):\n",
    "    with open(file, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    \n",
    "    get_nested_dict_num = lambda d: (len(d), *[len(v) for v in d.values()])\n",
    "    criterion = [get_nested_dict_num(item['criteria']) for item in data]\n",
    "    num_array = np.array(list(chain(*criterion)))\n",
    "    return np.unique(num_array, axis=0)\n",
    "\n",
    "\n",
    "for directory in os.listdir('dataset/T2IS'):\n",
    "    for file in ['train_metadata.jsonl', 'test_metadata.jsonl']:\n",
    "        if os.path.isfile(os.path.join('dataset/T2IS', directory, file)):\n",
    "            print(f\"{directory}/{file}: \", end='')\n",
    "            print(check_criteria_num(os.path.join('dataset/T2IS', directory, file)))\n",
    "\n",
    "# check_criteria_num('dataset/T2IS/.jsonl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common-use",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
